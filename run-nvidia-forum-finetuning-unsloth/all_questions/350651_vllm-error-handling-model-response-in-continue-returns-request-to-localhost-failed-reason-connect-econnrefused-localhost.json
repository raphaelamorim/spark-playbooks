{
  "post_stream": {
    "posts": [
      {
        "id": 1710877,
        "name": "Adamfriedmanbme",
        "username": "adamfriedmanbme",
        "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
        "created_at": "2025-11-08T06:40:36.090Z",
        "cooked": "<p>Trying to configure a vllm server on an ascent gx10 to serve a model which I can access through continue in vscode running on a workstation (macos). I am able to successfully run the “Install and Use vLLM for Inference” playbook on the gx10. On my workstation, using nvidia sync, I can open a terminal and successfully run:<br>\ncurl <a href=\"http://localhost:8000/v1/chat/completions\" rel=\"noopener nofollow ugc\">http://localhost:8000/v1/chat/completions</a><br>\n-H “Content-Type: application/json”<br>\n-d ‘{<br>\n“model”: “Qwen/Qwen2.5-Math-1.5B-Instruct”,<br>\n“messages”: [{“role”: “user”, “content”: “12*17”}],<br>\n“max_tokens”: 500<br>\n}’</p>\n<p>Using nvidia sync to run vscode on my workstation, I have a working ssh tunnel to the gx10. In the continue extension I have the config.yaml file setup like this:<br>\nname: Config</p>\n<p>version: 1.0.0</p>\n<p>schema: v1</p>\n<p>assistants:</p>\n<pre><code> - name: default\n\n model: OllamaSpark\n</code></pre>\n<p>models:</p>\n<pre><code> - name: OllamaSpark\n\n provider: ollama\n\n model: gpt-oss:120b\n\n apiBase: http://SPARK_IP:11434\n\n title: gpt-oss:120b\n\n roles:\n\n      - chat\n\n      - edit\n\n      - autocomplete\n\n - name: Qwen 2.5 Math\n\n provider: vllm\n\n model: qwen2.5-math-1.5b-instruct\n\n apiBase: [http://localhost:8000/v1/](http://localhost:8000/v1/)\n\n title: qwen2.5-math-1.5b-instruct\n\n roles:\n\n      - chat\n</code></pre>\n<p>In the continue extension I am able to select the Qwen 2.5 Math model, but when I enter a query (i.e. 12*17 like in the playbook example), I get an error pop up in the continue extension:<br>\n“Error handling model response”<br>\nthe error outputs:<br>\nrequest to <a href=\"http://localhost:8000/v1/chat/completions\" rel=\"noopener nofollow ugc\">http://localhost:8000/v1/chat/completions</a> failed, reason: connect ECONNREFUSED localhost:8000</p>\n<p>I think this may be an issue with permissions in macos. Would it be better for me to do this through remote access setup or is localhost fine? And do I need to open a port for firewall access like in the gpt-oss:120b playbook example? Ultimately, I am not really interested in running this qwen model in particular but I want to know the right way to configure continue to use different models through vllm running on DGX Spark.</p>\n<p>Any advice would be appreciated<br>\nThanks</p>",
        "post_number": 1,
        "post_type": 1,
        "posts_count": 3,
        "updated_at": "2025-11-08T06:40:36.090Z",
        "reply_count": 0,
        "reply_to_post_number": null,
        "quote_count": 0,
        "incoming_link_count": 18,
        "reads": 17,
        "readers_count": 16,
        "score": 73.2,
        "yours": false,
        "topic_id": 350651,
        "topic_slug": "vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost",
        "display_username": "Adamfriedmanbme",
        "primary_group_name": null,
        "flair_name": null,
        "flair_url": null,
        "flair_bg_color": null,
        "flair_color": null,
        "flair_group_id": null,
        "badges_granted": [],
        "version": 1,
        "can_edit": false,
        "can_delete": false,
        "can_recover": false,
        "can_see_hidden_post": false,
        "can_wiki": false,
        "link_counts": [
          {
            "url": "http://localhost:8000/v1/chat/completions",
            "internal": false,
            "reflection": false,
            "clicks": 0
          }
        ],
        "read": true,
        "user_title": null,
        "bookmarked": false,
        "actions_summary": [],
        "moderator": false,
        "admin": false,
        "staff": false,
        "user_id": 4636607,
        "hidden": false,
        "trust_level": 1,
        "deleted_at": null,
        "user_deleted": false,
        "edit_reason": null,
        "can_view_edit_history": false,
        "wiki": false,
        "post_url": "/t/vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost/350651/1",
        "event": null,
        "calendar_details": [],
        "can_accept_answer": false,
        "can_unaccept_answer": false,
        "accepted_answer": false,
        "topic_accepted_answer": true,
        "can_vote": false,
        "can_translate": false
      },
      {
        "id": 1711361,
        "name": "Adamfriedmanbme",
        "username": "adamfriedmanbme",
        "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
        "created_at": "2025-11-10T06:03:43.645Z",
        "cooked": "<p>I was able to work through the issues with vllm and got the server working correctly with continue. I’ll post the steps I used to get devstral 2507 running in case anyone wants to do something similar with vllm.</p>\n<ol>\n<li>set hugging face token variable<br>\nexport HUGGING_FACE_HUB_TOKEN=””</li>\n<li>pull docker image<br>\ndocker pull <a href=\"http://nvcr.io/nvidia/vllm:25.09-py3\" rel=\"noopener nofollow ugc\">nvcr.io/nvidia/vllm:25.09-py3</a></li>\n<li>run docker image<br>\ndocker run --gpus all -it --rm -p 8000:8000 --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 -v ~/.cache/huggingface:/root/.cache/huggingface -v ~/.cache/vllm:/root/.cache/vllm --env HUGGING_FACE_HUB_TOKEN=“$HUGGING_FACE_HUB_TOKEN” --env HF_HUB_DISABLE_XET=1 --env LC_ALL=C.UTF-8 --env LANG=C.UTF-8 --env PYTHONUTF8=1 --env PYTHONIOENCODING=utf-8 <a href=\"http://nvcr.io/nvidia/vllm:25.09-py3\" rel=\"noopener nofollow ugc\">nvcr.io/nvidia/vllm:25.09-py3</a> bash -c “hf download mistralai/Devstral-Small-2507 &amp;&amp; vllm serve mistralai/Devstral-Small-2507 --host 0.0.0.0 --port 8000 --gpu-memory-utilization 0.9 --max-model-len 32768 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral”</li>\n<li>use nvidia sync to open a terminal on your workstation and check if the model can be called<br>\n<a href=\"http://0.0.0.0:8000/v1/chat/completions\" rel=\"noopener nofollow ugc\">http://0.0.0.0:8000/v1/chat/completions</a> -H “Content-Type: application/json” -d ‘{“model”: “mistralai/Devstral-Small-2507”, “messages”: [{“role”: “user”, “content”: “What is the capital of France?”}], “max_tokens”: 100}’</li>\n<li>using nvidia sync open vscode, click on continue extension and open config.yaml. I have my config set like this for devstral:<br>\nmodels:\n<ul>\n<li>name: Devstral Small 2507<br>\nprovider: vllm<br>\napiBase: http://&lt;DGX_SPARK_IP&gt;:8000/v1<br>\nmodel: mistralai/Devstral-Small-2507<br>\ntitle: mistralai/Devstral-Small-2507<br>\nroles:<br>\n- chat<br>\n- edit<br>\n- apply</li>\n</ul>\n</li>\n<li>in the continue extension reload local config to update settings</li>\n</ol>\n<p>A couple notes: xet downloads from hugging face do not work with the nvidia docker image. I ran into a weird text encoder issue when making calls to hugging face which is why those UTF8 settings need to be declared in the run command. If you run into networking issue try allowing firewall access on port 8000 on the dgx spark. On your workstation make sure network permissions in security settings are correct. Also make sure you connect to nvidia sync with your &lt;DGX_SPARK_IP&gt;, you can’t use the .local method, and IP address in the config.yaml has to be the same.</p>",
        "post_number": 2,
        "post_type": 1,
        "posts_count": 3,
        "updated_at": "2025-11-10T06:03:43.645Z",
        "reply_count": 0,
        "reply_to_post_number": null,
        "quote_count": 0,
        "incoming_link_count": 0,
        "reads": 15,
        "readers_count": 14,
        "score": 2.8,
        "yours": false,
        "topic_id": 350651,
        "topic_slug": "vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost",
        "display_username": "Adamfriedmanbme",
        "primary_group_name": null,
        "flair_name": null,
        "flair_url": null,
        "flair_bg_color": null,
        "flair_color": null,
        "flair_group_id": null,
        "badges_granted": [],
        "version": 1,
        "can_edit": false,
        "can_delete": false,
        "can_recover": false,
        "can_see_hidden_post": false,
        "can_wiki": false,
        "link_counts": [
          {
            "url": "http://nvcr.io/nvidia/vllm:25.09-py3",
            "internal": false,
            "reflection": false,
            "clicks": 0
          },
          {
            "url": "http://0.0.0.0:8000/v1/chat/completions",
            "internal": false,
            "reflection": false,
            "clicks": 0
          }
        ],
        "read": true,
        "user_title": null,
        "bookmarked": false,
        "actions_summary": [],
        "moderator": false,
        "admin": false,
        "staff": false,
        "user_id": 4636607,
        "hidden": false,
        "trust_level": 1,
        "deleted_at": null,
        "user_deleted": false,
        "edit_reason": null,
        "can_view_edit_history": false,
        "wiki": false,
        "post_url": "/t/vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost/350651/2",
        "event": null,
        "can_accept_answer": false,
        "can_unaccept_answer": false,
        "accepted_answer": true,
        "topic_accepted_answer": true,
        "can_translate": false
      },
      {
        "id": 1719714,
        "name": "system",
        "username": "system",
        "avatar_template": "/user_avatar/forums.developer.nvidia.com/system/{size}/68080_2.png",
        "created_at": "2025-11-24T06:04:11.058Z",
        "cooked": "<p>This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.</p>",
        "post_number": 3,
        "post_type": 3,
        "posts_count": 3,
        "updated_at": "2025-11-24T06:04:11.058Z",
        "reply_count": 0,
        "reply_to_post_number": null,
        "quote_count": 0,
        "incoming_link_count": 0,
        "reads": 2,
        "readers_count": 1,
        "score": 0,
        "yours": false,
        "topic_id": 350651,
        "topic_slug": "vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost",
        "display_username": "system",
        "primary_group_name": null,
        "flair_name": null,
        "flair_url": null,
        "flair_bg_color": null,
        "flair_color": null,
        "flair_group_id": null,
        "badges_granted": [],
        "version": 1,
        "can_edit": false,
        "can_delete": false,
        "can_recover": false,
        "can_see_hidden_post": false,
        "can_wiki": false,
        "read": true,
        "user_title": "Moderator",
        "title_is_group": false,
        "bookmarked": false,
        "actions_summary": [],
        "moderator": true,
        "admin": true,
        "staff": true,
        "user_id": -1,
        "hidden": false,
        "trust_level": 4,
        "deleted_at": null,
        "user_deleted": false,
        "edit_reason": null,
        "can_view_edit_history": false,
        "wiki": false,
        "action_code": "autoclosed.enabled",
        "post_url": "/t/vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost/350651/3",
        "event": null,
        "can_accept_answer": false,
        "can_unaccept_answer": false,
        "accepted_answer": false,
        "topic_accepted_answer": true,
        "can_translate": false
      }
    ],
    "stream": [
      1710877,
      1711361,
      1719714
    ]
  },
  "timeline_lookup": [
    [
      1,
      18
    ],
    [
      2,
      16
    ],
    [
      3,
      2
    ]
  ],
  "suggested_topics": [],
  "tags": [],
  "tags_descriptions": {},
  "fancy_title": "vLLM error handling model response in Continue returns: “request to localhost failed, reason: connect ECONNREFUSED localhost”",
  "id": 350651,
  "title": "vLLM error handling model response in Continue returns: “request to localhost failed, reason: connect ECONNREFUSED localhost”",
  "posts_count": 3,
  "created_at": "2025-11-08T06:40:36.011Z",
  "views": 59,
  "reply_count": 0,
  "like_count": 0,
  "last_posted_at": "2025-11-10T06:03:43.645Z",
  "visible": true,
  "closed": true,
  "archived": false,
  "has_summary": false,
  "archetype": "regular",
  "slug": "vllm-error-handling-model-response-in-continue-returns-request-to-localhost-failed-reason-connect-econnrefused-localhost",
  "category_id": 721,
  "word_count": 723,
  "deleted_at": null,
  "user_id": 4636607,
  "featured_link": null,
  "pinned_globally": false,
  "pinned_at": null,
  "pinned_until": null,
  "image_url": null,
  "slow_mode_seconds": 0,
  "draft": null,
  "draft_key": "topic_350651",
  "draft_sequence": null,
  "unpinned": null,
  "pinned": false,
  "current_post_number": 1,
  "highest_post_number": 3,
  "deleted_by": null,
  "actions_summary": [
    {
      "id": 4,
      "count": 0,
      "hidden": false,
      "can_act": false
    },
    {
      "id": 8,
      "count": 0,
      "hidden": false,
      "can_act": false
    },
    {
      "id": 10,
      "count": 0,
      "hidden": false,
      "can_act": false
    },
    {
      "id": 7,
      "count": 0,
      "hidden": false,
      "can_act": false
    }
  ],
  "chunk_size": 20,
  "bookmarked": false,
  "topic_timer": null,
  "message_bus_last_id": 4,
  "participant_count": 1,
  "show_read_indicator": false,
  "thumbnails": null,
  "slow_mode_enabled_until": null,
  "related_topics": [
    {
      "fancy_title": "Vllm client connection refused",
      "id": 349393,
      "title": "Vllm client connection refused",
      "slug": "vllm-client-connection-refused",
      "posts_count": 10,
      "reply_count": 9,
      "highest_post_number": 12,
      "image_url": null,
      "created_at": "2025-10-29T05:37:08.808Z",
      "last_posted_at": "2025-10-31T01:13:09.003Z",
      "bumped": true,
      "bumped_at": "2025-10-31T01:13:09.003Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [
        "llama",
        "ai",
        "llama-31-8b-instruct"
      ],
      "tags_descriptions": {},
      "like_count": 0,
      "views": 68,
      "category_id": 740,
      "featured_link": null,
      "has_accepted_answer": true,
      "posters": [
        {
          "extras": null,
          "description": "Original Poster",
          "user": {
            "id": 209231,
            "username": "liu.jialu",
            "name": "",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/liu.jialu/{size}/146402_2.png",
            "trust_level": 1
          }
        },
        {
          "extras": "latest",
          "description": "Most Recent Poster, Accepted Answer",
          "user": {
            "id": 1427932,
            "username": "whitesscott",
            "name": "Whitesscott",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 34659,
            "username": "AastaLLL",
            "name": null,
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/aastalll/{size}/14043_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        }
      ]
    },
    {
      "fancy_title": "Vllm on spark cluster starts and loads model but API not running?",
      "id": 348196,
      "title": "Vllm on spark cluster starts and loads model but API not running?",
      "slug": "vllm-on-spark-cluster-starts-and-loads-model-but-api-not-running",
      "posts_count": 9,
      "reply_count": 3,
      "highest_post_number": 9,
      "image_url": null,
      "created_at": "2025-10-19T01:05:25.892Z",
      "last_posted_at": "2025-10-19T17:43:19.050Z",
      "bumped": true,
      "bumped_at": "2025-10-19T17:43:19.050Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [],
      "tags_descriptions": {},
      "like_count": 1,
      "views": 250,
      "category_id": 721,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": "latest",
          "description": "Original Poster, Most Recent Poster",
          "user": {
            "id": 4697637,
            "username": "stierma1",
            "name": "Stierma1",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 0
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 3191529,
            "username": "cosinus",
            "name": "Christian Otto Stelter",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/cosinus/{size}/447737_2.png",
            "trust_level": 1
          }
        }
      ]
    },
    {
      "fancy_title": "tensorrtserver: Detected NVIDIA GeForce 940MX GPU, which is not supported by this container",
      "id": 67659,
      "title": "tensorrtserver: Detected NVIDIA GeForce 940MX GPU, which is not supported by this container",
      "slug": "tensorrtserver-detected-nvidia-geforce-940mx-gpu-which-is-not-supported-by-this-container",
      "posts_count": 3,
      "reply_count": 0,
      "highest_post_number": 3,
      "image_url": null,
      "created_at": "2018-11-21T07:51:16.000Z",
      "last_posted_at": "2018-11-22T02:16:59.000Z",
      "bumped": true,
      "bumped_at": "2018-11-22T02:16:59.000Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [],
      "tags_descriptions": {},
      "like_count": 0,
      "views": 2087,
      "category_id": 92,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": "latest",
          "description": "Original Poster, Most Recent Poster",
          "user": {
            "id": 197325,
            "username": "125536673",
            "name": null,
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 192972,
            "username": "NVES",
            "name": null,
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/nves/{size}/14043_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        }
      ]
    },
    {
      "fancy_title": "Aunch NVIDIA NIM (llama3-8b-instruct) for LLMs locally",
      "id": 312389,
      "title": "Aunch NVIDIA NIM (llama3-8b-instruct) for LLMs locally",
      "slug": "aunch-nvidia-nim-llama3-8b-instruct-for-llms-locally",
      "posts_count": 4,
      "reply_count": 2,
      "highest_post_number": 4,
      "image_url": "https://global.discourse-cdn.com/nvidia/original/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6.jpeg",
      "created_at": "2024-11-06T10:05:46.421Z",
      "last_posted_at": "2024-11-08T19:58:33.211Z",
      "bumped": true,
      "bumped_at": "2024-11-08T19:58:33.211Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": [
        {
          "max_width": null,
          "max_height": null,
          "width": 1011,
          "height": 523,
          "url": "https://global.discourse-cdn.com/nvidia/original/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6.jpeg"
        },
        {
          "max_width": 800,
          "max_height": 800,
          "width": 800,
          "height": 413,
          "url": "https://global.discourse-cdn.com/nvidia/optimized/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6_2_800x413.jpeg"
        },
        {
          "max_width": 600,
          "max_height": 600,
          "width": 600,
          "height": 310,
          "url": "https://global.discourse-cdn.com/nvidia/optimized/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6_2_600x310.jpeg"
        },
        {
          "max_width": 400,
          "max_height": 400,
          "width": 400,
          "height": 206,
          "url": "https://global.discourse-cdn.com/nvidia/optimized/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6_2_400x206.jpeg"
        },
        {
          "max_width": 300,
          "max_height": 300,
          "width": 300,
          "height": 155,
          "url": "https://global.discourse-cdn.com/nvidia/optimized/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6_2_300x155.jpeg"
        },
        {
          "max_width": 200,
          "max_height": 200,
          "width": 200,
          "height": 103,
          "url": "https://global.discourse-cdn.com/nvidia/optimized/4X/e/7/a/e7a1414461371664e40543921ff84b4539db4ed6_2_200x103.jpeg"
        }
      ],
      "tags": [
        "nim",
        "llama3-8b-instruct"
      ],
      "tags_descriptions": {},
      "like_count": 0,
      "views": 180,
      "category_id": 699,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": "latest",
          "description": "Original Poster, Most Recent Poster",
          "user": {
            "id": 3831191,
            "username": "rawnak.kumar",
            "name": "Rawnak Kumar",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 1547621,
            "username": "neal.vaidya",
            "name": "Neal Vaidya",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/neal.vaidya/{size}/256893_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        }
      ]
    },
    {
      "fancy_title": "I&rsquo;d like to learn how to use the latest vLLM on DGX Spark",
      "id": 350254,
      "title": "I'd like to learn how to use the latest vLLM on DGX Spark",
      "slug": "id-like-to-learn-how-to-use-the-latest-vllm-on-dgx-spark",
      "posts_count": 9,
      "reply_count": 5,
      "highest_post_number": 9,
      "image_url": null,
      "created_at": "2025-11-05T08:59:17.617Z",
      "last_posted_at": "2025-11-06T16:33:34.152Z",
      "bumped": true,
      "bumped_at": "2025-11-06T16:33:34.152Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [
        "cuda"
      ],
      "tags_descriptions": {},
      "like_count": 3,
      "views": 517,
      "category_id": 721,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": null,
          "description": "Original Poster",
          "user": {
            "id": 4731106,
            "username": "zh_useai",
            "name": "Zh",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/zh_useai/{size}/449712_2.png",
            "trust_level": 0
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 4703451,
            "username": "raphael.amorim",
            "name": "Raphael Amorim",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/raphael.amorim/{size}/446733_2.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 1354318,
            "username": "swtb-datascience",
            "name": "",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 3567453,
            "username": "aniculescu",
            "name": "Aniculescu",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/aniculescu/{size}/382649_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        },
        {
          "extras": "latest",
          "description": "Most Recent Poster",
          "user": {
            "id": 4686603,
            "username": "rapha",
            "name": "Rapha",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/rapha/{size}/445064_2.png",
            "trust_level": 1
          }
        }
      ]
    },
    {
      "fancy_title": "VSS 2.3.0 Docker remote_llm_deployment Failed to generate TRT-LLM engine",
      "id": 333902,
      "title": "VSS 2.3.0 Docker remote_llm_deployment Failed to generate TRT-LLM engine",
      "slug": "vss-2-3-0-docker-remote-llm-deployment-failed-to-generate-trt-llm-engine",
      "posts_count": 6,
      "reply_count": 2,
      "highest_post_number": 7,
      "image_url": null,
      "created_at": "2025-05-21T12:48:32.560Z",
      "last_posted_at": "2025-05-23T06:46:08.643Z",
      "bumped": true,
      "bumped_at": "2025-05-23T06:46:08.643Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": true,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [
        "nim",
        "llama",
        "kosmos-2",
        "paligemma"
      ],
      "tags_descriptions": {},
      "like_count": 0,
      "views": 131,
      "category_id": 680,
      "featured_link": null,
      "has_accepted_answer": true,
      "posters": [
        {
          "extras": "latest",
          "description": "Original Poster, Most Recent Poster",
          "user": {
            "id": 3542424,
            "username": "pdemelo",
            "name": "Pdemelo",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster, Accepted Answer",
          "user": {
            "id": 1865205,
            "username": "yuweiw",
            "name": "Yuweiw",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/yuweiw/{size}/101255_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": -1,
            "username": "system",
            "name": "system",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/system/{size}/68080_2.png",
            "admin": true,
            "moderator": true,
            "trust_level": 4
          }
        }
      ]
    },
    {
      "fancy_title": "VSS local deployment single gpu: Failed to load VIA stream handler - Guardrails / CA-RAG setup failed",
      "id": 337943,
      "title": "VSS local deployment single gpu: Failed to load VIA stream handler - Guardrails / CA-RAG setup failed",
      "slug": "vss-local-deployment-single-gpu-failed-to-load-via-stream-handler-guardrails-ca-rag-setup-failed",
      "posts_count": 5,
      "reply_count": 2,
      "highest_post_number": 6,
      "image_url": null,
      "created_at": "2025-07-03T14:08:54.894Z",
      "last_posted_at": "2025-07-04T15:06:35.237Z",
      "bumped": true,
      "bumped_at": "2025-07-04T15:06:35.237Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": true,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [
        "nim",
        "llama",
        "llama-31-8b-instruct"
      ],
      "tags_descriptions": {},
      "like_count": 1,
      "views": 145,
      "category_id": 680,
      "featured_link": null,
      "has_accepted_answer": true,
      "posters": [
        {
          "extras": "latest",
          "description": "Original Poster, Most Recent Poster, Accepted Answer",
          "user": {
            "id": 4069506,
            "username": "O13K511",
            "name": "Mosolab",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/o13k511/{size}/390932_2.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 1865205,
            "username": "yuweiw",
            "name": "Yuweiw",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/yuweiw/{size}/101255_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": -1,
            "username": "system",
            "name": "system",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/system/{size}/68080_2.png",
            "admin": true,
            "moderator": true,
            "trust_level": 4
          }
        }
      ]
    },
    {
      "fancy_title": "vLLM on dual sparks",
      "id": 351413,
      "title": "vLLM on dual sparks",
      "slug": "vllm-on-dual-sparks",
      "posts_count": 3,
      "reply_count": 0,
      "highest_post_number": 3,
      "image_url": null,
      "created_at": "2025-11-14T11:33:15.261Z",
      "last_posted_at": "2025-11-16T14:49:45.501Z",
      "bumped": true,
      "bumped_at": "2025-11-16T14:49:45.501Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [],
      "tags_descriptions": {},
      "like_count": 2,
      "views": 126,
      "category_id": 721,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": null,
          "description": "Original Poster",
          "user": {
            "id": 105805,
            "username": "chbrain",
            "name": null,
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": "latest",
          "description": "Most Recent Poster",
          "user": {
            "id": 4703451,
            "username": "raphael.amorim",
            "name": "Raphael Amorim",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/raphael.amorim/{size}/446733_2.png",
            "trust_level": 1
          }
        }
      ]
    },
    {
      "fancy_title": "Install and Use vLLM for Inference on two Sparks does not work",
      "id": 349496,
      "title": "Install and Use vLLM for Inference on two Sparks does not work",
      "slug": "install-and-use-vllm-for-inference-on-two-sparks-does-not-work",
      "posts_count": 74,
      "reply_count": 60,
      "highest_post_number": 76,
      "image_url": null,
      "created_at": "2025-10-30T00:19:55.984Z",
      "last_posted_at": "2025-11-25T03:56:03.639Z",
      "bumped": true,
      "bumped_at": "2025-11-25T03:56:03.639Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [],
      "tags_descriptions": {},
      "like_count": 22,
      "views": 900,
      "category_id": 721,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": null,
          "description": "Original Poster",
          "user": {
            "id": 4721372,
            "username": "mark440",
            "name": "Mark",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 4042910,
            "username": "mpanthofer",
            "name": "Mpanthofer",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 4738271,
            "username": "gm112",
            "name": "Gm",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": null,
          "description": "Frequent Poster",
          "user": {
            "id": 4734015,
            "username": "yuan.ren.er",
            "name": "Yuan Ren Er",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": "latest",
          "description": "Most Recent Poster",
          "user": {
            "id": 1643100,
            "username": "PrinceHal",
            "name": "",
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/princehal/{size}/451063_2.png",
            "trust_level": 1
          }
        }
      ]
    },
    {
      "fancy_title": "vLLM container 25.10-py3 fails to start",
      "id": 349748,
      "title": "vLLM container 25.10-py3 fails to start",
      "slug": "vllm-container-25-10-py3-fails-to-start",
      "posts_count": 11,
      "reply_count": 9,
      "highest_post_number": 13,
      "image_url": null,
      "created_at": "2025-10-31T15:58:31.053Z",
      "last_posted_at": "2025-11-12T07:10:01.194Z",
      "bumped": true,
      "bumped_at": "2025-11-12T07:10:01.194Z",
      "archetype": "regular",
      "unseen": false,
      "pinned": false,
      "unpinned": null,
      "visible": true,
      "closed": false,
      "archived": false,
      "bookmarked": null,
      "liked": null,
      "thumbnails": null,
      "tags": [
        "generative_ai"
      ],
      "tags_descriptions": {},
      "like_count": 1,
      "views": 199,
      "category_id": 740,
      "featured_link": null,
      "has_accepted_answer": false,
      "posters": [
        {
          "extras": null,
          "description": "Original Poster",
          "user": {
            "id": 4685888,
            "username": "joost-de-v",
            "name": "Joost ",
            "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
            "trust_level": 1
          }
        },
        {
          "extras": "latest",
          "description": "Most Recent Poster",
          "user": {
            "id": 34659,
            "username": "AastaLLL",
            "name": null,
            "avatar_template": "/user_avatar/forums.developer.nvidia.com/aastalll/{size}/14043_2.png",
            "primary_group_name": "Employee",
            "flair_name": "Employee",
            "flair_group_id": 63,
            "moderator": true,
            "trust_level": 2
          }
        }
      ]
    }
  ],
  "summarizable": false,
  "accepted_answer": {
    "post_number": 2,
    "username": "adamfriedmanbme",
    "name": null,
    "excerpt": "I was able to work through the issues with vllm and got the server working correctly with continue. I’ll post the steps I used to get devstral 2507 running in case anyone wants to do something similar with vllm. \n\nset hugging face token variable \nexport HUGGING_FACE_HUB_TOKEN=””\npull docker image \nd&hellip;",
    "accepter_name": null,
    "accepter_username": "adamfriedmanbme"
  },
  "can_vote": false,
  "vote_count": 0,
  "user_voted": false,
  "discourse_zendesk_plugin_zendesk_id": null,
  "discourse_zendesk_plugin_zendesk_url": "https://your-url.zendesk.com/agent/tickets/",
  "details": {
    "can_edit": false,
    "notification_level": 1,
    "participants": [
      {
        "id": 4636607,
        "username": "adamfriedmanbme",
        "name": "Adamfriedmanbme",
        "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png",
        "post_count": 2,
        "primary_group_name": null,
        "flair_name": null,
        "flair_url": null,
        "flair_color": null,
        "flair_bg_color": null,
        "flair_group_id": null,
        "trust_level": 1
      }
    ],
    "created_by": {
      "id": 4636607,
      "username": "adamfriedmanbme",
      "name": "Adamfriedmanbme",
      "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png"
    },
    "last_poster": {
      "id": 4636607,
      "username": "adamfriedmanbme",
      "name": "Adamfriedmanbme",
      "avatar_template": "https://developer.download.nvidia.com/images/forums/profile-default-devtalk-84.png"
    }
  },
  "bookmarks": []
}